{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa6323-7c67-44e8-99a0-7857527a9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import albumentations.pytorch\n",
    "from glob import glob\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e5f2c-fd8a-4cbd-bf84-125a2f064578",
   "metadata": {},
   "source": [
    "В этот раз мы познакомимся с задачей детекции. Сделаем это, обучив модель YOLOv5. Это далеко не самая современная модель, но она достаточно близка к тем, которые используются сейчас.\n",
    "\n",
    "Всего есть два типа моделей детекции: двухэтапные и одноэтапные. Несколько примеров моделей, использующих разные подходы:\n",
    "- RCNN подход: Предполагает двухэтапный проход модели по изображению, где изначально выделяются зоны способами классического компьютерного зрения, а затем происходит уже знакомая нам классификация объектов в зонах и предсказание с помощью задачи регрессии координат box для объектов. \n",
    "- YOLO подход: Одноэтапный детектор удаляет процесс извлечения области интереса и напрямую классифицирует и регрессирует анкерные блоки-кандидаты (anchor-boxes). YOLO — это архитектура детекции, которая называется YOU ONLY LOOK ONCE.Она обучается от от начала до конца, для обработки изображения и прогнозирования ограничивающих рамок (BBox) и меток классов для каждой ограничивающей рамки напрямую. Наверное, наиболее популярная, также лицензия предполгает постоянное появление новых моделей\n",
    "- Detr - также одноэтапный, однако использующий трансформерную архитектуру и особенный дизайн предсказывающей головы, чтобы уйти от необходимости предсказания анкерных блоков (что довольно затратно), предсказывая объекты сразу. Иронично, но в поздних версиях к анкерам вернулись.\n",
    "\n",
    "![alt_text](../additional_materials/images/detection_stages.jfif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec2167-5e5d-495f-b72a-711cdc83699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acce64e-913c-4de7-926c-1ac42d64704c",
   "metadata": {},
   "source": [
    "Для начала посмотрим, как модель строит свои предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ae82a-929f-45ea-b842-05626d70b2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")\n",
    "im = \"https://ultralytics.com/images/zidane.jpg\"\n",
    "\n",
    "for f in \"zidane.jpg\", \"bus.jpg\":\n",
    "    torch.hub.download_url_to_file(\"https://ultralytics.com/images/\" + f, f)  # download 2 images\n",
    "im1 = Image.open(\"zidane.jpg\")  # PIL image\n",
    "im2 = cv2.imread(\"bus.jpg\")[..., ::-1]  # OpenCV image (BGR to RGB)\n",
    "\n",
    "results = model([im1, im2], size=640)  # batch of images\n",
    "results.print()\n",
    "results.show()\n",
    "\n",
    "results.xyxy[0]\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c5473-30fd-4c6b-8317-b9cada4043df",
   "metadata": {},
   "source": [
    "Другой способ загрузить предобученную модель - загрузить с хаба YOLO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33b43b-68df-4e0e-89ce-afac5f25a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd34812-a07a-474f-aac3-6b0cd78e4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model([im1, im2])  # batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7381146f-151d-4e1d-aec5-e8b59d876537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведите новые результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6a90e3-29a4-4783-88b6-59a0f46af172",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'supervision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msupervision\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_ultralytics(result)\n\u001b[0;32m      3\u001b[0m box_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mBoxAnnotator()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'supervision'"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "detections = sv.Detections.from_ultralytics(result)\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
    "\n",
    "annotated_image = image.copy()\n",
    "annotated_image = box_annotator.annotate(annotated_image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(annotated_image, detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d6f76-1bf9-4c58-b1da-774e6a93ebb1",
   "metadata": {},
   "source": [
    "Как устроена модель: \n",
    "\n",
    "- **Backbone** - это основная часть сети. Для YOLOv5 бекбон спроектирован с использованием CSP-Darknet53 — модификации архитектуры Darknet, использовавшейся в предыдущих версиях.\n",
    "- **Neck**: Эта часть соединяет backbone и head. В YOLOv5 используются структуры SPPF и New CSP-PAN.\n",
    "- **Head**: Эта часть отвечает за конечный результат. Она генерирует предсказанные bbox-ы.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c22efd-f6b7-42cc-a36c-650f13b1ec27",
   "metadata": {},
   "source": [
    "У YOLO длинная история. Рассмотрим некоторые основные идеи. \n",
    "![alt_text](../additional_materials/images/yolo_evolution.png)\n",
    "\n",
    "Мы сегодня рассмотрим основные вещи YOLOv5, попробуем ее и YOLOv11. \n",
    "Итак, общая архитектура YOLOv5 показана ниже:\n",
    "![alt_text](../additional_materials/images/YOLOv5-1.png).\n",
    "\n",
    "Какие важные идеи были использованы? \n",
    "Первое - SCPNet (Cross Stage Partial Network). Это важная часть, которая довольно долго тянется в YOLO для того, чтобы управлять протеканием градиентов. Чуть раньше перед этим была придумана DenseNet как ультра-версия резнета ( все блоки соединены со всеми), и из нее выросла идея конкатенировать выходы блоков с более поздними. Это было добавлено в YOLO4, показало эффективность и используется в некотором виде до сих пор. \n",
    "\n",
    "![alt_text](../additional_materials/images/yolo_back.png).\n",
    "\n",
    "Второе - Spatial Pyramid pooling. Этот вид пулинга берет входы с разных этапов прохода по сети и объединяет ыместе конкатенацией, что похволяет находить объекты разных размеров.\n",
    "\n",
    "![alt_text](../additional_materials/images/sppf.jfif)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60d284a-bd2f-4078-85d8-6b8defc3ddad",
   "metadata": {},
   "source": [
    "Обучение проводится за один шаг, а не в несколько этапов. В YOLOv5 функция лосса была следующей:\n",
    "![alt_text](../additional_materials/images/det_loss.svg)\n",
    "\n",
    "Она содержит: \n",
    "- Classes Loss (BCE Loss): Бинарная кросс-энтропия для классификации категорий\n",
    "- Objectness Loss (BCE Loss): Еще одна кросс-энтропия, которая позволяет определить, есть ли объект в предсказанном bbox-е.\n",
    "- Location Loss (CIoU Loss): Complete IoU loss, определяет ошибку локализации в гриде\n",
    "Кроме того, функция потерь взвешивается для разных размеров объектов: $L = w_{big} * L_{big} + w_{medium} * L_{medium} + w_{small} * L_{small} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58a0c24-617c-4f3c-9133-233c4d0a31ca",
   "metadata": {},
   "source": [
    "Как строятся обучающие датасеты? У YOLO свой формат,описывающий изображения следующим образом: \n",
    "Рассмотрим пример картинки: \n",
    "\n",
    "![alt_text](../additional_materials/images/two-persons-tie.avif)\n",
    "\n",
    "Она содержит два объекта. Каждому из них соответствует bbox и класс. \n",
    "Они сохраняются в файл с разметкой вида: \n",
    "<img src=\"../additional_materials/images/two-persons-tie-1.avif\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Датасет же организуется так как указано ниже:\n",
    "```\n",
    "../datasets/coco128/images/im0.jpg  # image\n",
    "../datasets/coco128/labels/im0.txt  # label\n",
    "```\n",
    "Другой часто использующийся формат датасета - COCO. Пример ниже:\n",
    "```json\n",
    "{\n",
    "    \"info\": {\n",
    "        \"description\": \"COCO 2017 Dataset\",\n",
    "        \"url\": \"http://cocodataset.org\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": 2017,\n",
    "        \"contributor\": \"COCO Consortium\",\n",
    "        \"date_created\": \"2017/09/01\"\n",
    "    },\n",
    "    \"licenses\": [\n",
    "        {\"url\": \"http://creativecommons.org/licenses/by/2.0/\",\"id\": 4,\"name\": \"Attribution License\"}\n",
    "    ],\n",
    "    \"images\": [\n",
    "        {\"id\": 242287, \"license\": 4, \"coco_url\": \"http://images.cocodataset.org/val2017/xxxxxxxxxxxx.jpg\", \"flickr_url\": \"http://farm3.staticflickr.com/2626/xxxxxxxxxxxx.jpg\", \"width\": 426, \"height\": 640, \"file_name\": \"xxxxxxxxx.jpg\", \"date_captured\": \"2013-11-15 02:41:42\"},\n",
    "        {\"id\": 245915, \"license\": 4, \"coco_url\": \"http://images.cocodataset.org/val2017/nnnnnnnnnnnn.jpg\", \"flickr_url\": \"http://farm1.staticflickr.com/88/xxxxxxxxxxxx.jpg\", \"width\": 640, \"height\": 480, \"file_name\": \"nnnnnnnnnn.jpg\", \"date_captured\": \"2013-11-18 02:53:27\"}\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\"id\": 125686, \"category_id\": 0, \"iscrowd\": 0, \"segmentation\": [[164.81, 417.51,......167.55, 410.64]], \"image_id\": 242287, \"area\": 42061.80340000001, \"bbox\": [19.23, 383.18, 314.5, 244.46]},\n",
    "        {\"id\": 1409619, \"category_id\": 0, \"iscrowd\": 0, \"segmentation\": [[376.81, 238.8,........382.74, 241.17]], \"image_id\": 245915, \"area\": 3556.2197000000015, \"bbox\": [399, 251, 155, 101]},\n",
    "        {\"id\": 1410165, \"category_id\": 1, \"iscrowd\": 0, \"segmentation\": [[486.34, 239.01,..........495.95, 244.39]], \"image_id\": 245915, \"area\": 1775.8932499999994, \"bbox\": [86, 65, 220, 334]}\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\"supercategory\": \"speaker\",\"id\": 0,\"name\": \"echo\"},\n",
    "        {\"supercategory\": \"speaker\",\"id\": 1,\"name\": \"echo dot\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03377157-4807-423b-b183-bb92518a6b9f",
   "metadata": {},
   "source": [
    "Как модели обучают? Можно, как обычно в torch, создать датасет и обучить модель, используя кастомные лоссы. Впрочем, в ultralitics есть готовые функции для этого, нужно лишь подготовить датасет.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fc9511-f548-40c8-9136-1be9e90f24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d2ee9-e05c-4e5a-b936-f3bf5afac986",
   "metadata": {},
   "source": [
    "Загрузим датасет. Это датасет, содержащий объекты и персонажей apex legends. Этот датасет - один из публичных датасетов roboflow, воспользуемся им."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a24a0-a0af-409a-a7bd-0326564a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YOUR KEY\")\n",
    "project = rf.workspace(\"roboflow-100\").project(\"apex-videogame\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443efe5-2aad-460c-854d-7a45da16e588",
   "metadata": {},
   "source": [
    "Дообучим модель. Обучать ее с нуля для нас нет смысла. Попробуем дообучить 2 эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf10ad1-cb34-496d-865e-3f5be8b16447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = model.train(data=\"apex-videogame-2/data.yaml\", epochs=2, imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9397554-5f01-4b6c-a23e-cb4377433172",
   "metadata": {},
   "source": [
    "Обратите внимание, как быстро обучается модель. Большой плюс YOLO - ее быстродействие. Результаты обучения сохраняются в файлы, их можно посмотреть после обучения. Посмотрим же. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f813701-49ee-4921-9f60-5a1c5d52986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "filename = r\"D:\\projects\\spbu_deep_learning\\notebooks\\runs\\detect\\train4\\results.png\"\n",
    "IPyImage(filename=filename, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05e30b-9c6c-403a-9a7b-a5c075e68f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = r\"D:\\projects\\spbu_deep_learning\\notebooks\\runs\\detect\\train4\\val_batch0_pred.jpg\"\n",
    "IPyImage(filename=filename, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30d468-51f1-446c-be3f-0f2404c2883e",
   "metadata": {},
   "source": [
    "Провалидируем модель: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2924e73-bea5-4c4d-b268-1d1f50491db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=val model=\"D:\\projects\\spbu_deep_learning\\notebooks\\runs\\detect\\train4\\weights\\best.pt\" data={dataset.location}/data.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301caa3-2ee5-43d4-a090-6d1f32172bca",
   "metadata": {},
   "source": [
    "Задание: сделайте инференс модели на тестовых данных. Напишите функцию инференса и вывод результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69497500-e500-48c8-a819-876bea0c4f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spbu_dl",
   "language": "python",
   "name": "spbu_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
