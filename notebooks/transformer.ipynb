{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7267592d-61f0-477e-8aea-a0adacf5eedb",
   "metadata": {},
   "source": [
    "# Transformer and question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58370c96-4db5-4f21-924c-06dd42709391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers huggingface_hub\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679a495-6dd9-4c92-8d2a-53ccc6398e82",
   "metadata": {},
   "source": [
    "Для начала опробуем библиотеку. Попробуем определять тональность текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3d3636-2786-4b3b-a0b8-a4c0cf337286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "classifier = transformers.pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8f681-e25d-40a6-a0d6-3354c6ec2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'Losyash': \"I just look like a moose, but at heart I'm a butterfly.\",\n",
    "    'Krosh': \"The sun is shining - good, not shining - also good, I am my own sun.\",\n",
    "    'Kar Karich': \"You can wait your whole life for the right moment and end up never saying something important to each other.\",\n",
    "    'Nyusha': 'If you are not destined to become Miss Universe, what is the point of preening at all?!'\n",
    "}\n",
    "\n",
    "outputs = {}# True if positive and False if negative\n",
    "assert sum(outputs.values()) == 2\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e89389-df34-481e-b4ef-270506636d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea9f95-b2db-49a3-9272-e31bdf0fc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Let's do tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e2d21-827b-438d-afea-b62d195cc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"Let's do tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17f53b-779d-4e6f-8f4b-a79984232619",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer(\"Let's do tokenization!\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11dd8a-f029-4c04-8ae8-4e82e8c209d5",
   "metadata": {},
   "source": [
    "token_type_ids и attention_mask — это дополнительные значения, которые могут пригодиться при использовании разных моделей. Например, если вы решаете задачу языкового моделирования, то наверняка захотите при помощи attention_mask замаскировать то, что модели надо предсказать (например, вторую половину предложения).\n",
    "\n",
    "[CLS] и [SEP] — это специальные токены, которые использует BERT. Первый используется для предсказания того, является ли часть B предложением, непосредственно следующим за частью A, а так же используется для обработки глобальной информации. Второй является токеном-разделителем. Токенизатор сам расставил их за нас в данном случае, но иногда приходится самостоятельно проставлять их руками.\n",
    "\n",
    "Обратите внимание, что каждая обученная модель использует свой токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a38d4f-74db-43f0-bb75-2c3af6e180c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = torch.tensor([[200, 200, 200]])\n",
    "sequence2_ids = torch.tensor([[200, 200]])\n",
    "batched_ids = torch.tensor(\n",
    "    [\n",
    "        [200, 200, 200],\n",
    "        [200, 200, tokenizer.pad_token_id],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(model(sequence1_ids).logits)\n",
    "print(model(sequence2_ids).logits)\n",
    "print(model(batched_ids).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccaec2d-882d-4660-b64d-93378b023a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_sequences = [\n",
    "    \"I am a robot and I hate humans\",\n",
    "    \"I am a human and i hate robots very much\",\n",
    "]\n",
    "batched_ids = tokenizer(batched_sequences)[\"input_ids\"]\n",
    "batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03337b45-1778-4ecf-8e29-70c001e9d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d167f-b114-407c-a8e7-33418d974c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae11792-3e0a-4b12-af12-540a230ba0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe93a76-18aa-499f-8a6e-53d6d5cf6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e97c63-3f01-485d-bb2d-f00d589ffb4c",
   "metadata": {},
   "source": [
    "В датасете есть несколько важных полей:\n",
    "\n",
    "- answers: местоположение первого токена ответа и текст ответа.\n",
    "- context: исходная информация, из которой модели необходимо извлечь ответ.\n",
    " - question: вопрос, на который должна ответить модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d18f4a-dce2-48eb-b039-3799d7eb6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = squad[\"train\"][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3a431-2771-4163-8a0d-cb8b7154ef46",
   "metadata": {},
   "source": [
    "То, что находится в поле контекста и вопроса, довольно понятно. Поле ответов немного сложнее, поскольку оно содержит словарь с двумя полями, каждое их которых, в свою очередь, - список. Это формат, который будет использоваться метрикой squad во время оценки. В кастомном случае можно организовывать эти поля как угодно. Поле text содержит ответ на вопрос, а поле answer_start содержит индекс начального символа каждого ответа в контексте.\n",
    "\n",
    "Во время обучения для каждого семпла есть только один ответ, при валидации же несколько (разной степени конкретности, например)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c96de-5673-45d8-b55a-12c01b3e47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da855bd1-4bc4-4ab3-9850-8fc0378e6be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sample[\"question\"], sample[\"context\"])\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557b762-fbb6-4fbd-a4ba-595c54bb0440",
   "metadata": {},
   "source": [
    "Для дообучения модели нам необходимо предобработать данные. \n",
    "У некоторых вопросов контекст слишком длинный, поэтому его нужно сократить до максимальной длины (в нашем случае 386). Для этого нужно установить truncation=\"only_second\".\n",
    "Затем нужно сопоставить начальную и конечную позиции ответа с исходным контекстом, установив return_offset_mapping=True.\n",
    "Чтобы потом определить, какая часть смещения соответствует вопросу, а какая — контексту, нужно использовать метод sequence_ids().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c61ee8-7b06-4e9e-a5ab-08c44d385790",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21d505-f5b7-47ac-b938-6d8a79375ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "        squad[\"train\"][2:6][\"question\"],\n",
    "        squad[\"train\"][2:6][\"context\"],\n",
    "        max_length=100,\n",
    "        truncation=\"only_second\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca704f9-4e09-4f2c-8d2c-f73d932bd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338b4f3-5223-4f35-b881-50080c001e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} samples.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c0fb4-4904-401b-b995-effa62e7298f",
   "metadata": {},
   "source": [
    "Посмотрим на ответы, которые мы должны получить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33da04-5192-4a72-a36f-f541e98aece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = squad[\"train\"][2:6][\"answers\"]\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba860a-9e3c-4462-aa26-69a9e014f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs[\"input_ids\"][0]), len(inputs[\"offset_mapping\"][0]), inputs[\"overflow_to_sample_mapping\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf11126-d5c1-4db4-a0cf-5cda08008101",
   "metadata": {},
   "source": [
    "Для того, чтобы можно было что-нибудь научить, мы должны каждому семплу поставить в соответствие необходимый ответ. Поэтому придется сделать дополнительный шаг предобработки, который похволит определитб для каждого семпла, содержит ли он ответ или нет, тогда мы должны возвращать особую комбинацию (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c391d5e-d1f5-47af-953c-32926cbbc95a",
   "metadata": {},
   "source": [
    "Чтобы определить, содержится ли ответ в семплах (или был отрезан), и, если необходимо, позиции его токенов, мы сначала должны найти индексы, которые начинают и заканчивают контекст во входных семплах. Для этого мы могли бы использовать идентификаторы типов токенов, но поскольку они не обязательно существуют для всех моделей (например, DistilBERT не требует их), вместо этого мы будем использовать метод Sequence_ids() BatchEncoding, возвращаемый нашим токенизатором.\n",
    "\n",
    "Получив эти индексы токенов, мы можем взять соответствующие смещения семплов, которые представляют собой кортежи из двух целых чисел, представляющих диапазон символов внутри исходного контекста. Таким образом, мы можем определить, начинается ли фрагмент контекста в этой функции после ответа или заканчивается до начала ответа (в этом случае метка равна (0, 0)). Если это не так, мы выполняем цикл, чтобы найти первый и последний токен ответа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb196a99-f75e-4eb9-96d9-330bffd0f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]  # Какому изначальному примеру соответствует семпл\n",
    "    answer = answers[sample_idx] # Нужный ответ\n",
    "    start_char = answer[\"answer_start\"][0] # Позиция его начала в исходной последовательности\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0]) \n",
    "    sequence_ids = inputs.sequence_ids(i) \n",
    "\n",
    "    # Определяем первую и последнюю позицию контекста во входе.\n",
    "    # None не относится ни к какому семпплу. 0 - вопрос, 1 - ответ\n",
    "    idx = 0\n",
    "    \n",
    "    # определите начало и конец контекста\n",
    "    # while sequence_ids[idx] != 1 ...\n",
    "    \n",
    "    # Если ответ не полностью въодит в контекст, то оставим (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        # your code\n",
    "    else:\n",
    "        # В противном случае сохраним первую и последнюю позицию ответа\n",
    "        idx = context_start\n",
    "        # найдите первую позицию ответа\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        # найдите последнюю позицию ответа\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44ba2d-ad6f-4a69-875e-5e074ed1e0b6",
   "metadata": {},
   "source": [
    "Проверим результат. Сравним значения, полученные с помощью предобработки с таргетами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f4c29-9918-4b84-95b3-870ab59fdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 # поиграйте с индексом - найдите семпл, где ответ влез\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"GT: {answer}, preprocessed: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83372bdc-5458-4adb-81e9-d4f2213670e7",
   "metadata": {},
   "source": [
    "Теперь мы можем написать функцию предобработки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0049b42-d91f-438d-bbee-43bca847c4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = # your code (set up tokenizer)\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        # your code - get start and end position \n",
    "        # add them to lists\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb299f-4547-4a39-8f5b-394cce64613b",
   "metadata": {},
   "source": [
    "Чтобы применить эту функцию ко всему обучающему набору, используем метод Dataset.map() с флагом batched=True. Здесь это необходимо, так как мы меняем длину  датасета за счет разбиения контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462f780-d05d-4acb-ba35-a100fce04e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfb497-c49c-4d41-9eaf-94f9a88c07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(squad[\"train\"]), len(tokenized_squad[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474188a1-7046-40d8-9c7c-a6930dd03ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186e05d-da4d-40d6-91d4-4a261c550f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e679d26-f417-40a6-bc44-7e8bd9f1f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a3cbd-51e9-4272-b581-1c45f8516f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47423c-9183-4348-9bb6-e307c9d65e67",
   "metadata": {},
   "source": [
    "Теперь мы можем приступить к обучению модели. У huggingface для этого есть класс trainer, который позволяет максимально легко обучать модели для типичных тасок. Все, о чем нужно позаботиться - это параметры конфига для обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9915a3-c59d-4069-96f0-e077e747eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"qa_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd362954-19ad-41c8-835a-3a99790fe4e6",
   "metadata": {},
   "source": [
    "Проверим нашу модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654653e-b65e-44e9-a1ba-f5d0e5269074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\", model=\"qa_model\")\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf36d7-d955-4ff6-8eed-80cf8f6c97d3",
   "metadata": {},
   "source": [
    "Задание - попробуйте разные модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29aba7e-b565-4517-821b-bc63ab5f9edc",
   "metadata": {},
   "source": [
    "Но на самом деле мы не ограничены базовым функционалом, поэтому все то же самое мы можем сделать и с помощью torch. Это не слишком отличается от обычного пайплайна обучения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c7a2bb-134d-434f-8190-1881d4487fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset = tokenized_squad[\"train\"]\n",
    "validation_dataset = tokenized_squad[\"test\"]\n",
    "\n",
    "# Преобразуем датасет в формат torch\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_dataset, collate_fn=default_data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be213d-3b9b-4abb-b9f4-2d3629a2fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522061e-302b-43d8-96ce-8622e731fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa684cf-a198-41af-a4c4-9518e49d0253",
   "metadata": {},
   "source": [
    "Настроим акселератор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5362aa4-d3ff-44c3-b693-b6f6424b8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "model, optimizer, train_dataloader, eval_dataloader = # prepare all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a16e71-ba87-4523-9522-d07bfeeca497",
   "metadata": {},
   "source": [
    "В этот раз мы не будем считать метрики, так как это потребует достаточно времени. Одюнако при желании вы можете их добавить, чтобы оценить качество модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f8fea9-ae32-4928-b7c9-6395da05622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # your code (classic one)\n",
    "        \n",
    "    # Eval\n",
    "    # <your code> - compute the validation loss\n",
    "    # Save and upload\n",
    "    # This is more advanced task, use the power of the internet\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f4b95-9845-410c-a3b4-ca55f288ad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spbu_dl",
   "language": "python",
   "name": "spbu_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
