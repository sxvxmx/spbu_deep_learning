{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a2d824-1692-4c75-b96b-e9fed55ba330",
   "metadata": {},
   "source": [
    "# Сегментация\n",
    "В этот раз мы рассмотрим чуть менее стандартную задачу - сегментации изображений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4742d-cb19-40cf-99d0-ff48ecafd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import albumentations.pytorch\n",
    "from glob import glob\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4869b-6e3e-4052-8f0a-1a551b3618f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r\"D:\\data\\spbu_dl\\aeroscapes\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dca0f3-2e65-42e5-b911-c4dc52f5b9db",
   "metadata": {},
   "source": [
    "Для начала рассмотрим датасет изображений, полученных с дрона на высоте от 5 до 50 метров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c42cd-e15b-4a8f-8a1a-c10f04823853",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['Background', 'Person', 'Bike', 'Car', 'Drone', 'Boat', 'Animal', 'Obstacle', 'Construction', 'Vegetation', 'Road', 'Sky']\n",
    "\n",
    "COLOR_MAPPING = {\n",
    "    (0, 0, 0): 0,  # Background\n",
    "    (192, 128, 128): 1,      # Person\n",
    "    (0, 128, 0): 2,          # Bike\n",
    "    (128, 128, 128): 3,      # Car\n",
    "    (128, 0, 0): 4,          # Drone\n",
    "    (0, 0, 128): 5,          # Boat\n",
    "    (192, 0, 128): 6,        # Animal\n",
    "    (192, 0, 0): 7,          # Obstacle\n",
    "    (192, 128, 0): 8,        # Construction\n",
    "    (0, 64, 0): 9,           # Vegetation\n",
    "    (128, 128, 0): 10,       # Road\n",
    "    (0, 128, 128): 11,       # Sky\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b4d2b-6f07-4a43-952d-973b0cdcf3c3",
   "metadata": {},
   "source": [
    "Сегментация - это та же классификация. Разница с обычным кейсом лишь в том, что классифицируется каждый пиксель изображенияю Поэтому вместо приятной структуры с папками нам придется вернуться к рукописным датасетам.\n",
    "\n",
    "Создадим датасет: он будет читать файлы из папок с изображениями и масками. Маски трехмерные, но мы знаем их маппинг в классы. Так что легко можно поменять маску на номер желаемого класса. Остальное стандартно - трансформируем семпл, возвращаем результат.\n",
    "\n",
    "Сейчас немного усложним задачу, будем читать из папки Visualizations, а не SegmentationMasks (но вы можете и оттуда сразу читать, конечно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb41de1-e050-42cd-9bbd-cc30671226ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialSegmentationDataset(Dataset):\n",
    "    def __init__(self, root, data = \"trn\", transformations = None):\n",
    "        files = open(f\"{root}/ImageSets/{data}.txt\", \"r\").read().split(\"\\n\")\n",
    "        self.im_paths, self.gt_paths = self.get_data_paths(root = root, files = files) \n",
    "        self.transformations = transformations\n",
    "        self.n_cls = 11\n",
    "        assert len(self.im_paths) == len(self.gt_paths)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im = self.read_image(self.im_paths[idx])\n",
    "        mask = cv2.imread(self.gt_paths[idx], cv2.IMREAD_COLOR)\n",
    "        mask = convert_mask(mask)\n",
    "        if self.transformations: \n",
    "            # Проведите трансформации\n",
    "        return im, mask\n",
    "        \n",
    "    def get_data_paths(self, root, files) -> tuple[list[str], list[str]]: \n",
    "        images = [\n",
    "            path for path in sorted(glob(f\"{root}/JPEGImages/*.jpg\")) if self.get_fname(path) in files]\n",
    "        masks =  # Получите пути и для масок\n",
    "        return images, masks \n",
    "       \n",
    "    def get_fname(self, path): \n",
    "        return os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "    def read_image(self, path): \n",
    "        return cv2.cvtColor(\n",
    "            cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB\n",
    "        )\n",
    "\n",
    "def convert_mask(mask_rgb):\n",
    "    mask = np.zeros((mask_rgb.shape[0], mask_rgb.shape[1]), dtype=np.uint8)\n",
    "    for rgb, idx in COLOR_MAPPING.items():\n",
    "        mask[(mask_rgb == rgb).all(axis=2)] = idx\n",
    "    return mask\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b0aa9-3050-4201-a021-caad827a495d",
   "metadata": {},
   "source": [
    "Создадим инстанс датасета для визуализации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942f51b-b477-43be-b763-5e5d7c562a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_h, im_w = 256, 256\n",
    "transformations = A.Compose( [A.Resize(im_h, im_w),ToTensorV2(transpose_mask = True) ])\n",
    "\n",
    "train_dataset = AerialSegmentationDataset(root=base_dir, data=\"trn\", transformations=transformations)\n",
    "test_dataset = AerialSegmentationDataset(root=base_dir, data=\"val\", transformations=transformations)\n",
    "n_cls = train_dataset.n_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62e69e-c1f8-4edb-87db-2e09d2af0e89",
   "metadata": {},
   "source": [
    "Добавим функции для того, чтобы красиво нарисовать наши примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e087bc-cbfc-43ff-85b5-d610d6c4b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(rows, cols, count, im, gt = None, title = \"Original Image\"):\n",
    "    plt.subplot(rows, cols, count)\n",
    "    image = im.squeeze(0).int().permute(1,2,0) if not gt else im.squeeze(0).float()\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\"); plt.title(title)\n",
    "    return count + 1\n",
    "    \n",
    "def visualize(ds, n_ims):\n",
    "    plt.figure(figsize = (25, 20))\n",
    "    rows = n_ims // 4; cols = n_ims // rows\n",
    "    count = 1\n",
    "    indices = [random.randint(0, len(ds)) for _ in range(n_ims)]\n",
    "    for idx, index in enumerate(indices):\n",
    "        if count == n_ims + 1: break\n",
    "        im, gt = ds[index]\n",
    "        # First Plot\n",
    "        count = plot(rows, cols, count, im = im)\n",
    "        # Second Plot\n",
    "        count = plot(rows, cols, count, im = gt, gt = True, title=\"Segmentation mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7943bbf9-ed4e-4c39-838e-2beefeb19c6b",
   "metadata": {},
   "source": [
    "И нарисуем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1b4514-aec7-4f78-a55d-a9a9a0d611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(train_dataset, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685a723-2e7b-49a8-a9a7-416a40698b71",
   "metadata": {},
   "source": [
    "Дополнительное задание: выведите исходные пары изображение - трехканальная маска"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f111051d-e150-49f0-a29f-a98a3a441c74",
   "metadata": {},
   "source": [
    "Теперь уже создадим наш датасет, а также разобьем его на трейн-валидацию-тест\n",
    "\n",
    "**Допзадание**: попробуйте еще другие аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd742d00-6dc5-49fc-9526-268a1747d5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "transformations = A.Compose( [\n",
    "    A.Resize(im_h, im_w),\n",
    "    A.augmentations.transforms.Normalize(mean = mean, std = std), \n",
    "    ToTensorV2(transpose_mask = True)\n",
    "])\n",
    "\n",
    "split = [0.9, 0.1]\n",
    "batch_size = 16\n",
    "\n",
    "# создайте датасеты для теста и валидации\n",
    "\n",
    "train_len = # Определите обучающую длину\n",
    "val_len = # Определите валидационную длину\n",
    "\n",
    "# Data split\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
    "    \n",
    "print(f\"\\nThere are {len(train_dataset)} number of images in the train set\")\n",
    "print(f\"There are {len(val_dataset)} number of images in the validation set\")\n",
    "print(f\"There are {len(test_dataset)} number of images in the test set\\n\")\n",
    "\n",
    "# Get dataloaders\n",
    "train_dataloader  = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle = False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66844c2f-9085-49bd-abce-527229463441",
   "metadata": {},
   "source": [
    "Теперь создадим нашу архитектуру. Это [UNet](https://arxiv.org/pdf/1505.04597) - одна из базовых архитектур для сегментацию, аналоги которой еще и активно используются в самых разных приложениях.\n",
    "\n",
    "Задание: дополните код, чтобы получить UNet с двумя внутренними блоками. \n",
    "Минимальная задача для слабых компьютеров - максимальное число фичей - 128. В таком случае мы будем использовать только по два блока, а не 4 исходных.\n",
    "\n",
    "Исходная архитектура:\n",
    "![alt_text](../additional_materials/images/u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5d2f2-3965-4c46-9e49-350f3c537ec9",
   "metadata": {},
   "source": [
    "# Как реализовать up-conv?\n",
    "В torch для этого есть nn.TransposeConv2d. \n",
    "На самом деле это не совсем свертка - она берет входные значения и вначале создает что-то вроде буфера, где хранит результаты умножения ядра на вход поэлементно. После того, как все результаты получены, они собираются воедино суммированием (можно посмотреть подробное объяснение [здесь](https://stackoverflow.com/questions/69782823/understanding-the-pytorch-implementation-of-conv2dtranspose):\n",
    "\n",
    "![alt_text](../additional_materials/images/tc1.png)\n",
    "\n",
    "Примеры:\n",
    "![alt_text](../additional_materials/images/tc2.png)\n",
    "\n",
    "![alt_text](../additional_materials/images/tc3.png)\n",
    "\n",
    "![alt_text](../additional_materials/images/tc4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c823fb1-793b-4b01-b56f-47c49b1e170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    # YOUR CODE OF BLOCK\n",
    "    # Note that the hxw must stay the same\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,\n",
    "        out_channels: int = 1,\n",
    "        features: List[int] = [32, 64, 128],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.down = nn.ModuleList()\n",
    "        self.up = nn.ModuleList()\n",
    "        self.pool =  # Phase change - from down to up\n",
    "        # append blocks:\n",
    "            # append blocks to down part\n",
    "            # features x2 in out\n",
    "        #append blocks in reversed order:\n",
    "            # append pairs of [transposed convolutions and blocks to up)\n",
    "            # features /2 in out\n",
    "        self.bottleneck = # your code\n",
    "        \n",
    "        self.final_conv = # your code\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        skip_connections = []  # storage for skip connections\n",
    "        for down in self.down:\n",
    "            # add forwards and also fill skip connections\n",
    "            \n",
    "        x = # add bottleneck\n",
    "        skip_connections = # reverse list of skip connections\n",
    "        for idx in range(0, len(self.up), 2):\n",
    "            # run up and also get corresponding skip\n",
    "            # if shape of x and skipped x are different, don't forget to reshape x\n",
    "            concat_skip = # Concat skip and x along channels dimensions (b, c, h, w)\n",
    "            x = # call next block with skip\n",
    "        return # call final conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fac1f-4b3f-4990-9dec-b966b21a2e51",
   "metadata": {},
   "source": [
    "Создадим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4a892-c485-44cd-87f0-fd190f20add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(out_channels=n_cls+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a51c37-a9bc-44e2-94e2-ae96a18659b1",
   "metadata": {},
   "source": [
    "Теперь немного подушним с метриками. Для сегментации и детекции они похожи, так как обе задачи требуют определения целевых регионов. Поэтому наиболее естественны метрики, основанные на геометрии.\n",
    "Рассмотрим две: IoU и Dice. \n",
    "![alt_text](../additional_materials/images/metrics.png)\n",
    "\n",
    "Так как у нас задача мультиклассовая, мы должны посчитать эти метрики по классам и усреднить результат.\n",
    "\n",
    "**Задание**: Дополните код, реализовав метрики mIoU и Dice.\n",
    "\n",
    "\n",
    "Для того, чтобы можно было в будущем сочетать наши рукописные метрики и торчовые, напишем обертку, которая хранит в себе словарь со всем метриками сразу. Каждая рукописная должна иметь те же методы, что и torchmetrics.Metric: __call__ и compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311f33b-26f8-47f9-90f3-daaf18622983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable, Dict\n",
    "\n",
    "class Metric(ABC):\n",
    "    def __init__(self):\n",
    "        self._values = []\n",
    "\n",
    "    def __call__(self, preds: Iterable, targets: Iterable) -> float:\n",
    "        value = self.compute_metric(preds, targets)\n",
    "        self._values.append(value)\n",
    "        return value\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_metric(self, preds: Iterable, targets: Iterable) -> float:\n",
    "        pass\n",
    "\n",
    "    def compute(self) -> float:\n",
    "        if not self._values:\n",
    "            return 0.0\n",
    "        mean_value = sum(self._values) / len(self._values)\n",
    "        return mean_value\n",
    "\n",
    "    def reset(self):\n",
    "        self._values = []\n",
    "\n",
    "\n",
    "class MetricCollection:\n",
    "    def __init__(self, metrics: Dict[str, Metric | torchmetrics.Metric]):\n",
    "        if not isinstance(metrics, dict):\n",
    "            raise TypeError(\"metrics must be a dictionary of Metric instances.\")\n",
    "        for name, metric in metrics.items():\n",
    "            if not isinstance(metric, Metric) and not isinstance(metric, torchmetrics.Metric):\n",
    "                raise TypeError(\n",
    "                    f\"Value for '{name}' is not an instance of Metric (handmade or torchmetrics).\"\n",
    "                )\n",
    "        self.metrics = metrics\n",
    "\n",
    "    def __call__(self, preds: Iterable, targets: Iterable) -> Dict[str, float]:\n",
    "        computed_values = {}\n",
    "        # Call metrics one by one \n",
    "        return computed_values\n",
    "\n",
    "    def compute(self) -> Dict[str, float]:\n",
    "        return # compute metrics one by one\n",
    "\n",
    "    def reset(self):\n",
    "        # reset metrics one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f1032-b2b5-4bf4-ba05-275fd6c0b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanIntersectionOverUnion(Metric):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def compute_metric(self, preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        with torch.no_grad():\n",
    "            iou_per_class = []\n",
    "            for c in range(self.num_classes):\n",
    "                # compute IoU\n",
    "        return np.nanmean(iou_per_class)\n",
    "\n",
    "class DiceScore(Metric):\n",
    "    def __init__(self, num_classes: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.eps = eps\n",
    "\n",
    "    def compute_metric(self, preds: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        # If preds are in one-hot format, convert them to class indices\n",
    "        if preds.dim() == targets.dim() + 1:\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for c in range(self.num_classes):\n",
    "            # compute dice\n",
    "        mean_dice = np.mean(iou_per_class)\n",
    "        return mean_dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5ef8d-bad4-49c0-b724-e94f46fac379",
   "metadata": {},
   "source": [
    "Зададим лосс и оптимизатор. У нас еще задача не очень сложная, можем воспользоваться кросс-энтропией.\n",
    "\n",
    "**Доп.задание**: создайте лосс из Dice Score. Обучите модель с новым лоссом. Как меняется результат? Объясните, почему можно сделать лосс из Dice Score, но не выйдет из IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80170b-5140-4c60-a9f3-76a595f4ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=0.001, weight_decay=0.3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914657a-4779-4b21-bc29-2b1062e0e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection({\"mIoU\": MeanIntersectionOverUnion(n_cls), \"dice\": DiceScore(n_cls)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b1919-223a-436d-bd3a-ada4188a1138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: MetricCollection, \n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    binary: int | None = None,\n",
    "    return_losses=False,\n",
    "):\n",
    "    model = model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            # YOUR CODE\n",
    "            # train step: preds, loss, optimization, etc            \n",
    "            metrics_desc = \",\".join([f\"{i}: {v:.4}\" for i,v in metrics_dict.items()])\n",
    "            # update description for tqdm\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \" + metrics_desc\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "    metric_values = metrics.compute()\n",
    "    if return_losses:\n",
    "        return metric_values, all_losses\n",
    "    else:\n",
    "        return metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb987c-a577-43ec-baf4-c5950d6f5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: nn.Module,\n",
    "        data_loader: torch.utils.data.DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        metrics: MetricCollection,\n",
    "        binary: bool | None = None,\n",
    "        device: str = \"cuda:0\"):\n",
    "    model = model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with tqdm.tqdm(total=len(data_loader)) as prbar:\n",
    "        for inputs, targets in data_loader:\n",
    "            with torch.no_grad():\n",
    "               # YOUR CODE\n",
    "            metrics_desc = \",\".join([f\"{i}: {v:.4}\" for i,v in metrics_val.items()])\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {loss.item():.4} \" + metrics_desc\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "    return {**metrics.compute(), \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7a2ad-6489-415e-b887-a71422c844ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    train_data_loader: torch.utils.data.DataLoader,\n",
    "    validation_data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    metrics: MetricCollection,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler=None,\n",
    "    binary: int | None = None,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    all_train_losses = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_eval_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        # construct iterators\n",
    "        train_iterator = iter(train_data_loader)\n",
    "        validation_iterator = iter(validation_data_loader)\n",
    "        # train step\n",
    "        print(f\"Train Epoch: {epoch}\")\n",
    "        train_metrics, one_epoch_train_losses = train_epoch(\n",
    "            model=model,\n",
    "            data_loader=train_iterator,\n",
    "            criterion=criterion,\n",
    "            metrics=metrics,\n",
    "            optimizer=optimizer,\n",
    "            binary=binary,\n",
    "            return_losses=True,\n",
    "        )\n",
    "        # save train losses\n",
    "        all_train_losses.extend(one_epoch_train_losses)\n",
    "        epoch_train_losses.append(train_metrics[\"loss\"])\n",
    "        # eval step\n",
    "        print(f\"Validation Epoch: {epoch}\")\n",
    "\n",
    "        # call function validation epoch and\n",
    "        # save eval losses\n",
    "        with torch.no_grad():\n",
    "            validation_metrics = validate(\n",
    "                model=model,\n",
    "                data_loader=validation_iterator,\n",
    "                criterion=criterion,\n",
    "                metrics=metrics,\n",
    "                binary=binary,\n",
    "                device=device\n",
    "            )\n",
    "        # save eval losses\n",
    "        epoch_eval_losses.append(validation_metrics[\"loss\"])\n",
    "        # scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    return LossInfo(all_train_losses, epoch_train_losses, epoch_eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263328b-648c-4c8a-8bed-0f9d759faca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "_ = train(\n",
    "    model=model,\n",
    "    epochs=5,\n",
    "    train_data_loader=train_dataloader,\n",
    "    validation_data_loader=val_dataloader,\n",
    "    criterion=loss,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_classes=n_cls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2f852-e8d4-4a59-9a4a-f72387a3ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_img(\n",
    "    net: nn.Module, img: torch.Tensor, device: str = device, out_threshold: float = 0.5\n",
    ") -> np.ndarray:\n",
    "    net.eval()\n",
    "    net.to(device)\n",
    "\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = net(img)\n",
    "        probs = torch.softmax(output)\n",
    "        full_mask = probs.cpu().squeeze()\n",
    "\n",
    "        return (full_mask > out_threshold).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f4223-2791-4d8b-a782-2916dd08d435",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_dataset[0]\n",
    "predict_img(model, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072b8c16-8815-4b4a-9635-dea0b0799dcf",
   "metadata": {},
   "source": [
    "**Задание**: напишите функцию инференса и прогоните модель на тестовом датасете. Также используйте метрики, чтобы узнать точность в таком полубоевом режиме. Как именно организовать инференс - ваше решение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1762e5e7-ce19-4e36-b8ec-84bccbe88f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spbu_dl",
   "language": "python",
   "name": "spbu_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
